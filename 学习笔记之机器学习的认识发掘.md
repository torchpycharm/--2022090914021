# 机器学习的自我认识发掘

### Pytorch

PyTorch是一个针对深度学习，并且使用GPU和CPU来优化的张量库

调用Python接口可以通过Anaconda或Pip的方式安装，调用C++接口可直接下载对应的二进制库。对PyTorch比较熟悉了，也可以通过源码直接编译。 PyTorch是基于以下两个目的而打造的python科学计算框架：

PyTorch是基于以下两个目的而打造的python科学计算框架：


    (1). 无缝替换NumPy，并且通过利用GPU的算力来实现神经网络的加速。

    (2). 通过自动微分机制，让神经网络的实现变得更加容易

    张量(Tensor)可以想作数组和矩阵，是一种特殊的数据结构。在PyTorch中，神经网络的输入、输出以及网络的参数等数据，都是使用张量来进行描述。


### 机器学习

#### 1.

#### 近似理解：高中所学习的回归方程拟合数据，机器学习可能就是一种大数据的回归拟合？从数据中获得潜在客观规律的过程

#### 2.

人工智能>机器学习>深度学习

#### 3.监督学习：

读入数据集，判断类别，大致的值，也就是一种回归拟合

#### 4.无监督学习：

聚类算法，自动分类  甚至不需要指定类别，如大数据

推荐视频 

#### 5.深度学习：

神经网络：输入的数input 经中间的神经网络得到输出数output，收集到的数据增多，深度学习随之兴起

#### 6.损失函数：

如loss函数损失函数（loss function）是用来估量模型的预测值f(x)与真实值Y的不一致程度，它是一个非负实值函数,通常使用L(Y, f(x))来表示，损失函数越小，模型的鲁棒性就越好

涉及一些求导等复杂问题，也是一种拟合方式

#### 7.神经元/输入层/隐藏层/输出层/卷积：

构成神经网络的基本结构

![avatar](C:\Users\24557\Desktop\第七章.png)

可以用矩阵的乘法来理解神经网络的传播过程

从输入层到隐藏层再到输出层传递，即向前传播函数

#### 8.激活函数

常用的有sigmoid，tanh，relu，leaky relu，elu，softmax，swish

**一些激活函数的性质**

**非线性：**

当激活函数是非线性的，一个两层的神经网络就可以基本上逼近所有的函数。但如果激活函数是恒等激活函数的时候，即f(x) = x，就不满足这个性质，而且如果MLP使用的是恒等激活函数，那么其实整个网络跟单层神经网络是等价的。

**可微性：**

当优化方法是基于梯度的时候，就体现的该本质。

**单调性：**

当激活函数是单调的时候，单层网络能够保证是凸函数。
输出值的范围：当激活函数输出值是有限的时候，基于梯度的优化方法会更加稳定，因为特征的表示受有限权值的影响更显著；当激活函数的输出是无限的时候，模型的训练更加高效，不过这种情况很小，一般需要更小的learning rate。

**tip：激活函数是干嘛的**

如果不用激活函数，每一层节点的输入都是上层输出的线性函数，很容易验证，无论神经网络有多少层，输出都是输入的线性组合，与没有隐藏层效果相当，这种情况可以理解为最原始的感知机，那么网络的逼近能力就相当有限。因此引入非线性函数作为激活函数，这样深层神经网络表达能力就更加强大（不再是输入的线性组合，而是几乎可以逼近任意函数）。

![avatar](C:\Users\24557\Desktop\第二章.png)



![avatar](C:\Users\24557\Desktop\激活函数展示.png)

*两个激活函数的展示*

## 在做招新题时的感悟

​        其实在很早的时候就开始关注焦糖的招新题了，听了各种各样的工作室的招新宣讲以后，对焦糖工作室的印象最深刻，可能是“西点屋”的亲切外号，也可能是小冰箱的诱惑，让我重点关注了jotang。但那时候的我基本上还算一个计算机白痴，拿着一本c primer plus，看完c语言的数据类型的时候已经云里雾里了，再去看招新题，先是完全懵逼：什么是makedown？一个app嘛，在哪里可以下载啊？github是什么高深玩意儿，为什么我访问不了这个网址，向学长学姐打听的时候明显感受到了他们的一时语塞（一定是被我的瓜问题蠢到了吧hhh）再往下看招新题的时候，本来就没有的自信心再次被摧残---那么多个题，我甚至没能看懂任何一句话在说什么。特别是在看到人工智能方向的招新题时，还在想是什么样子的大佬才能做完如此复杂的题目。于是加入工作室的计划被我暂时搁置了。

​        之后的很长一段时间里我被c语言老师课程压力压得喘不过气来，入门慢的我只能靠翁恺网课的加持，以及额外一本c语言书（c primer plus）来支持我去做icoding的题，有段时间甚至只想学习线代与微积分，一眼编程都不想看，这也算是我进入大学的一个适应期吧。

​         真正开始着手加入工作室的时候是国庆节，由于回到家中，一个多月没见面的父母天天白天都会带着我到处跑，聚餐等，所以我每天的学习时间只有晚上，在小房间里从9点坐到凌晨两三点，前两天是痛苦的抉择方向，因为一直没有明确方向，我看了很多介绍视频，并仔细地阅读每一项招新题，最终我选择机器学习这个方向。

​         接下来就是为期三天痛苦的环境配置———下载了python，但想在vs上配置，搞了半天不行，看网上说用vscode或者pycharm，又开始研究这两者的好坏，最终选择了pycharm，此时又看到建议使用anaconda管理python环境，我一头雾水，anaconda是啥，环境是啥，为什么用c语言的时候没干这些事，抱着这些疑问，我开始在b站大学与CSDN上搜索信息，却因为视频看的太多，不凑巧没有找到正确的方法，反倒把自己绕进去了，先是删除了原有的python解释器，没能在anaconda官网上下载成功，后来使用了清华镜像源，弯路走来走去，终于是把anaconda和pycharm都安装好了，心想着这题是python语言，还是得赶紧学一学python。

​         几天的学习后，国庆结束，返校，我想办法打开了之前一直文件加载失败的代码框架，却感觉自己之前学了一个假的python，题目完全看不懂，甚至连开头的模块都下载不了，这时才发现torch模块需要专门在pytorch官网上下载，进入官网又傻眼了，为什么是一行指令，经查找才知道是要在cmd或者anaconda prompt上下载，一番折腾，放弃了原先的python基础学习进度，最终开始做题的时候，已经只剩下五天时间。

​        之后的每一天便是睡觉前最后一件事想想代码怎么填写，起床第一件事就是想为什么那行代码报错or警告了呢，在CSDN学习神经网络知识，了解其结构，运作方式，慢慢的我把所有的空填充完毕，但看到红色波浪线逐渐消失，我为这几天的努力感到由衷的满足，但一运行，直接报错一直说我NET not defined ，两天，我查阅各种资料，问各路大佬，都没有能成功解决我的问题。有一天跟我一个学java的学长聊天的时候，他问我python是不是要特别注意缩进的问题，当时我没有反应过来只是说对呀，但第二天跟一个同学交流的时候，发现我拷贝代码框架的时候不知出了什么问题，很多缩进都是乱的，调整以后总算解决了问题，但其他问题随之跳出，在不断地设置断点调试之后，我理解了实例化时输入层到隐藏层到输出层如何传递数据，如何用relu函数激活函数，训练网络如何与前面搭建的神经网络联系起来，怎样补充参数才合理，理解了之前半懂不懂从csdn上提取出来的函数用法到底是什么意思。最终那副拟合函数图像完美地从屏幕上显示出来的时候我长长地舒了一口气，本来就要放弃，截止的前一天犯了胃炎发着低烧，却还是挺过来，完成了一个小题，虽然没能完成之后的计算机视觉，没能达到完成度要求，但我还是觉得，无悔此遭，能通过自己的努力与不断地学习而完成一个目标，真的是一件很有意义的事情，非常感谢我选择了焦糖，也感激途中帮助过我的小伙伴们。

​                                        **凡是过往，皆为序章；生如逆旅，一苇以航**

![avatar](C:\Users\24557\Documents\Tencent Files\2455799399\FileRecv\图片\Screenshot_20221014_224942.jpg)

![avatar](C:\Users\24557\Documents\Tencent Files\2455799399\FileRecv\图片\Screenshot_20221014_221200.jpg)

![avatar](C:\Users\24557\Desktop\QQ图片20221014230323.png)
